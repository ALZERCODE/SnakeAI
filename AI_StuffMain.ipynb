{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Import Dependencies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Gym Stuff\n",
    "import gym\n",
    "from gym import Env\n",
    "from gym.spaces import Discrete, Box, Dict, Tuple, MultiBinary, MultiDiscrete\n",
    "\n",
    "# Import helpers\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "\n",
    "# Import stable baselines stuff\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import VecFrameStack\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv\n",
    "from stable_baselines3.common.evaluation import evaluate_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Game Stuff\n",
    "from snakeGame import SnakeGame"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Building the ENV**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SnakeENV(Env):\n",
    "        \n",
    "    def __init__(self, num_envs=1) -> None:\n",
    "        super(SnakeENV, self).__init__()\n",
    "        metadata = {'render.modes': ['human',]}\n",
    "\n",
    "        self.num_envs = num_envs\n",
    "        self.game = [SnakeGame() for _ in range(self.num_envs)]\n",
    "\n",
    "        self.action_space = Discrete(4)\n",
    "        self.observation_space = Dict([('head_pos', Box(low=np.array([0, 0]), high=np.array([self.game.window.width, self.game.window.height]))),\n",
    "                                        ('fruit_pos', Box(low=np.array([0, 0]), high=np.array([self.game.window.width, self.game.window.height]))),\n",
    "                                        ('length', Box(low=np.array([len(self.game.snake.body)]), high=np.array([self.game.window.width * self.game.window.height]))),\n",
    "                                        ('direction', Discrete(4))\n",
    "                                ])\n",
    "\n",
    "        self.time_run = 0\n",
    "\n",
    "    def isEpisodeLenght(self):\n",
    "        self.time_run += 1\n",
    "        return self.time_run > 6600\n",
    "\n",
    "    def step(self, action, render=True):\n",
    "        #done = self.isEpisodeLenght()\n",
    "\n",
    "        direction = self.action_to_direction(action)\n",
    "        reward, done = self.game.main(direction, render=render, isAI=True)\n",
    "\n",
    "        info = {}\n",
    "\n",
    "        return self.state, reward, done, info\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        self.game.render(mode=mode)\n",
    "\n",
    "    def reset(self):\n",
    "        self.game = SnakeGame()\n",
    "        self.time_run = 0\n",
    "        self.state = {'head_pos': np.array(self.game.snake.head_pos).astype(int),\n",
    "                        'fruit_pos': np.array(self.game.fruit.pos).astype(int),\n",
    "                        'length': np.array(len(self.game.snake.body)).astype(int),\n",
    "                        'direction': self.direction_to_action(self.game.snake.direction)\n",
    "                    }\n",
    "\n",
    "                    \n",
    "        return self.state\n",
    "\n",
    "\n",
    "    def action_to_direction(self, action):\n",
    "        if action == 0:\n",
    "            return 'UP'\n",
    "        elif action == 1:\n",
    "            return 'DOWN'\n",
    "        elif action == 2:\n",
    "            return 'LEFT'\n",
    "        elif action == 3:\n",
    "            return 'RIGHT'\n",
    "    def direction_to_action(self, direction):\n",
    "        if direction == 'UP':\n",
    "            return 0\n",
    "        elif direction == 'DOWN':\n",
    "            return 1\n",
    "        elif direction == 'LEFT':\n",
    "            return 2\n",
    "        elif direction == 'RIGHT':\n",
    "            return 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1 Score: -499\n",
      "Episode: 2 Score: -492\n",
      "Episode: 3 Score: -495\n",
      "Episode: 4 Score: -500\n",
      "Episode: 5 Score: -497\n"
     ]
    }
   ],
   "source": [
    "env = SnakeENV()\n",
    "episodes = 5\n",
    "\n",
    "\n",
    "for episode in range(1, episodes+1):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "\n",
    "    while not done:\n",
    "        env.render()\n",
    "        action = env.action_space.sample()\n",
    "        n_state, reward, done, info = env.step(action)\n",
    "        score += reward\n",
    "    print(f'Episode: {episode} Score: {score}')\n",
    "env.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Train a PPO model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "del env\n",
    "#env = SnakeENV()\n",
    "cpu_cores = 4\n",
    "#env = DummyVecEnv([lambda: SnakeENV])\n",
    "#env = SubprocVecEnv([lambda: SnakeENV for i in range(cpu_cores)])\n",
    "#env = VecFrameStack(env, n_stack=4)\n",
    "\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "env = make_vec_env(SnakeENV, n_envs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "log_path = \"./Training/Logs/\"\n",
    "model = PPO(\"MultiInputPolicy\", env, verbose=1, tensorboard_log=log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to ./Training/Logs/PPO_15\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 3.84     |\n",
      "|    ep_rew_mean     | -497     |\n",
      "| time/              |          |\n",
      "|    fps             | 55       |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 147      |\n",
      "|    total_timesteps | 8192     |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-5:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/alex/miniforge3/envs/gameAI/lib/python3.8/threading.py\", line 932, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/Users/alex/miniforge3/envs/gameAI/lib/python3.8/site-packages/tensorboard/summary/writer/event_file_writer.py\", line 233, in run\n",
      "    self._record_writer.write(data)\n",
      "  File \"/Users/alex/miniforge3/envs/gameAI/lib/python3.8/site-packages/tensorboard/summary/writer/record_writer.py\", line 40, in write\n",
      "    self._writer.write(header + header_crc + data + footer_crc)\n",
      "  File \"/Users/alex/miniforge3/envs/gameAI/lib/python3.8/site-packages/tensorboard/compat/tensorflow_stub/io/gfile.py\", line 766, in write\n",
      "    self.fs.append(self.filename, file_content, self.binary_mode)\n",
      "  File \"/Users/alex/miniforge3/envs/gameAI/lib/python3.8/site-packages/tensorboard/compat/tensorflow_stub/io/gfile.py\", line 160, in append\n",
      "    self._write(filename, file_content, \"ab\" if binary_mode else \"a\")\n",
      "  File \"/Users/alex/miniforge3/envs/gameAI/lib/python3.8/site-packages/tensorboard/compat/tensorflow_stub/io/gfile.py\", line 164, in _write\n",
      "    with io.open(filename, mode, encoding=encoding) as f:\n",
      "FileNotFoundError: [Errno 2] No such file or directory: b'./Training/Logs/PPO_15/events.out.tfevents.1671911463.Alzers-M1-Pro.local.25987.0'\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model\u001b[39m.\u001b[39;49mlearn(total_timesteps\u001b[39m=\u001b[39;49m\u001b[39m100000\u001b[39;49m)\n",
      "File \u001b[0;32m~/miniforge3/envs/gameAI/lib/python3.8/site-packages/stable_baselines3/ppo/ppo.py:299\u001b[0m, in \u001b[0;36mPPO.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps)\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlearn\u001b[39m(\n\u001b[1;32m    287\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    288\u001b[0m     total_timesteps: \u001b[39mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    296\u001b[0m     reset_num_timesteps: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    297\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mPPO\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 299\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m(PPO, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49mlearn(\n\u001b[1;32m    300\u001b[0m         total_timesteps\u001b[39m=\u001b[39;49mtotal_timesteps,\n\u001b[1;32m    301\u001b[0m         callback\u001b[39m=\u001b[39;49mcallback,\n\u001b[1;32m    302\u001b[0m         log_interval\u001b[39m=\u001b[39;49mlog_interval,\n\u001b[1;32m    303\u001b[0m         eval_env\u001b[39m=\u001b[39;49meval_env,\n\u001b[1;32m    304\u001b[0m         eval_freq\u001b[39m=\u001b[39;49meval_freq,\n\u001b[1;32m    305\u001b[0m         n_eval_episodes\u001b[39m=\u001b[39;49mn_eval_episodes,\n\u001b[1;32m    306\u001b[0m         tb_log_name\u001b[39m=\u001b[39;49mtb_log_name,\n\u001b[1;32m    307\u001b[0m         eval_log_path\u001b[39m=\u001b[39;49meval_log_path,\n\u001b[1;32m    308\u001b[0m         reset_num_timesteps\u001b[39m=\u001b[39;49mreset_num_timesteps,\n\u001b[1;32m    309\u001b[0m     )\n",
      "File \u001b[0;32m~/miniforge3/envs/gameAI/lib/python3.8/site-packages/stable_baselines3/common/on_policy_algorithm.py:268\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps)\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlogger\u001b[39m.\u001b[39mrecord(\u001b[39m\"\u001b[39m\u001b[39mtime/time_elapsed\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mint\u001b[39m(time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstart_time), exclude\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtensorboard\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    267\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlogger\u001b[39m.\u001b[39mrecord(\u001b[39m\"\u001b[39m\u001b[39mtime/total_timesteps\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_timesteps, exclude\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtensorboard\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 268\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlogger\u001b[39m.\u001b[39;49mdump(step\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_timesteps)\n\u001b[1;32m    270\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m    272\u001b[0m callback\u001b[39m.\u001b[39mon_training_end()\n",
      "File \u001b[0;32m~/miniforge3/envs/gameAI/lib/python3.8/site-packages/stable_baselines3/common/logger.py:459\u001b[0m, in \u001b[0;36mLogger.dump\u001b[0;34m(self, step)\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[39mfor\u001b[39;00m _format \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_formats:\n\u001b[1;32m    458\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(_format, KVWriter):\n\u001b[0;32m--> 459\u001b[0m         _format\u001b[39m.\u001b[39;49mwrite(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname_to_value, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname_to_excluded, step)\n\u001b[1;32m    461\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname_to_value\u001b[39m.\u001b[39mclear()\n\u001b[1;32m    462\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname_to_count\u001b[39m.\u001b[39mclear()\n",
      "File \u001b[0;32m~/miniforge3/envs/gameAI/lib/python3.8/site-packages/stable_baselines3/common/logger.py:366\u001b[0m, in \u001b[0;36mTensorBoardOutputFormat.write\u001b[0;34m(self, key_values, key_excluded, step)\u001b[0m\n\u001b[1;32m    363\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwriter\u001b[39m.\u001b[39madd_image(key, value\u001b[39m.\u001b[39mimage, step, dataformats\u001b[39m=\u001b[39mvalue\u001b[39m.\u001b[39mdataformats)\n\u001b[1;32m    365\u001b[0m \u001b[39m# Flush the output to the file\u001b[39;00m\n\u001b[0;32m--> 366\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwriter\u001b[39m.\u001b[39;49mflush()\n",
      "File \u001b[0;32m~/miniforge3/envs/gameAI/lib/python3.8/site-packages/torch/utils/tensorboard/writer.py:1200\u001b[0m, in \u001b[0;36mSummaryWriter.flush\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1198\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m   1199\u001b[0m \u001b[39mfor\u001b[39;00m writer \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mall_writers\u001b[39m.\u001b[39mvalues():\n\u001b[0;32m-> 1200\u001b[0m     writer\u001b[39m.\u001b[39;49mflush()\n",
      "File \u001b[0;32m~/miniforge3/envs/gameAI/lib/python3.8/site-packages/torch/utils/tensorboard/writer.py:150\u001b[0m, in \u001b[0;36mFileWriter.flush\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mflush\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    146\u001b[0m     \u001b[39m\"\"\"Flushes the event file to disk.\u001b[39;00m\n\u001b[1;32m    147\u001b[0m \u001b[39m    Call this method to make sure that all pending events have been written to\u001b[39;00m\n\u001b[1;32m    148\u001b[0m \u001b[39m    disk.\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 150\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mevent_writer\u001b[39m.\u001b[39;49mflush()\n",
      "File \u001b[0;32m~/miniforge3/envs/gameAI/lib/python3.8/site-packages/tensorboard/summary/writer/event_file_writer.py:121\u001b[0m, in \u001b[0;36mEventFileWriter.flush\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mflush\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    116\u001b[0m     \u001b[39m\"\"\"Flushes the event file to disk.\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \n\u001b[1;32m    118\u001b[0m \u001b[39m    Call this method to make sure that all pending events have been\u001b[39;00m\n\u001b[1;32m    119\u001b[0m \u001b[39m    written to disk.\u001b[39;00m\n\u001b[1;32m    120\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 121\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_async_writer\u001b[39m.\u001b[39;49mflush()\n",
      "File \u001b[0;32m~/miniforge3/envs/gameAI/lib/python3.8/site-packages/tensorboard/summary/writer/event_file_writer.py:176\u001b[0m, in \u001b[0;36m_AsyncWriter.flush\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_closed:\n\u001b[1;32m    175\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mIOError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mWriter is closed\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 176\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_byte_queue\u001b[39m.\u001b[39;49mjoin()\n\u001b[1;32m    177\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_writer\u001b[39m.\u001b[39mflush()\n",
      "File \u001b[0;32m~/miniforge3/envs/gameAI/lib/python3.8/queue.py:89\u001b[0m, in \u001b[0;36mQueue.join\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mall_tasks_done:\n\u001b[1;32m     88\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39munfinished_tasks:\n\u001b[0;32m---> 89\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mall_tasks_done\u001b[39m.\u001b[39;49mwait()\n",
      "File \u001b[0;32m~/miniforge3/envs/gameAI/lib/python3.8/threading.py:302\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[39mtry\u001b[39;00m:    \u001b[39m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    301\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 302\u001b[0m         waiter\u001b[39m.\u001b[39;49macquire()\n\u001b[1;32m    303\u001b[0m         gotit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    304\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.learn(total_timesteps=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#for i in range(100000):\n",
    "#    actions, _states = model.predict(model.get_env().reset())\n",
    "#    for j in range(env.num_envs):\n",
    "#        model.learn(actions[j])\n",
    "#        env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_path = \"./Training/Models/PPO_Snake_Model_MoreData\"\n",
    "model.save(ppo_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Eval and Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_path = \"./Training/Models/PPO_Snake_Model_MoreData\"\n",
    "\n",
    "env = SnakeENV()\n",
    "model = PPO.load(ppo_path, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_policy(model, env, n_eval_episodes=1, render=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gameAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "15f7727ce8d2895f6ec9d60196c112ed0686943606574ac3f5c99d53edfa75c1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
