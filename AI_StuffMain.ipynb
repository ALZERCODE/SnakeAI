{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Import Dependencies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Gym Stuff\n",
    "import gym\n",
    "from gym import Env\n",
    "from gym.spaces import Discrete, Box, Dict, Tuple, MultiBinary, MultiDiscrete\n",
    "\n",
    "# Import helpers\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "\n",
    "# Import stable baselines stuff\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import VecFrameStack\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv \n",
    "from stable_baselines3.common.evaluation import evaluate_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Game Stuff\n",
    "from snakeGame import SnakeGame"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Building the ENV**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SnakeENV(Env):\n",
    "        \n",
    "    def __init__(self) -> None:\n",
    "        #super().__init__()\n",
    "        self.game = SnakeGame()\n",
    "\n",
    "        self.action_space = Discrete(4)\n",
    "        self.observation_space = Dict([('head_pos', Box(low=np.array([0, 0]), high=np.array([self.game.window.width, self.game.window.height]))),\n",
    "                                        ('fruit_pos', Box(low=np.array([0, 0]), high=np.array([self.game.window.width, self.game.window.height]))),\n",
    "                                        ('length', Box(low=np.array([0]), high=np.array([np.inf]))),\n",
    "                                        ('direction', Discrete(4))\n",
    "                                ])\n",
    "\n",
    "\n",
    "    def step(self, action):\n",
    "        direction = self.action_to_direction(action)\n",
    "        reward, done = self.game.main(direction, render=False)\n",
    "\n",
    "        info = {}\n",
    "\n",
    "        return self.state, reward, done, info\n",
    "\n",
    "    def render(self):\n",
    "        self.game.render()\n",
    "\n",
    "    def reset(self):\n",
    "        self.game = SnakeGame()\n",
    "        self.state = {'head_pos': np.array(self.game.snake.head_pos).astype(int),\n",
    "                        'fruit_pos': np.array(self.game.fruit.pos).astype(int),\n",
    "                        'lenght': np.array(len(self.game.snake.body)).astype(int),\n",
    "                        'direction': self.direction_to_action(self.game.snake.direction)\n",
    "                    }\n",
    "        return self.state\n",
    "\n",
    "\n",
    "    def action_to_direction(self, action):\n",
    "        if action == 0:\n",
    "            return 'UP'\n",
    "        elif action == 1:\n",
    "            return 'DOWN'\n",
    "        elif action == 2:\n",
    "            return 'LEFT'\n",
    "        elif action == 3:\n",
    "            return 'RIGHT'\n",
    "    def direction_to_action(self, direction):\n",
    "        if direction == 'UP':\n",
    "            return 0\n",
    "        elif direction == 'DOWN':\n",
    "            return 1\n",
    "        elif direction == 'LEFT':\n",
    "            return 2\n",
    "        elif direction == 'RIGHT':\n",
    "            return 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1 Score: -1\n",
      "Episode: 2 Score: -1\n",
      "Episode: 3 Score: -1\n",
      "Episode: 4 Score: -1\n",
      "Episode: 5 Score: -1\n"
     ]
    }
   ],
   "source": [
    "env = SnakeENV()\n",
    "episodes = 5\n",
    "\n",
    "\n",
    "for episode in range(1, episodes+1):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "\n",
    "    while not done:\n",
    "        env.render()\n",
    "        action = env.action_space.sample()\n",
    "        n_state, reward, done, info = env.step(action)\n",
    "        score += reward\n",
    "    print(f'Episode: {episode} Score: {score}')\n",
    "env.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Train a PPO model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "del env\n",
    "env = SnakeENV()\n",
    "#env = VecFrameStack(env, n_stack=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Error: unknown policy type CNNPolicy,the only registed policy type are: ['MlpPolicy', 'CnnPolicy', 'MultiInputPolicy']!\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [45], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m log_path \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m./Training/Logs/\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m model \u001b[39m=\u001b[39m PPO(\u001b[39m\"\u001b[39;49m\u001b[39mCNNPolicy\u001b[39;49m\u001b[39m\"\u001b[39;49m, env, verbose\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, tensorboard_log\u001b[39m=\u001b[39;49mlog_path)\n",
      "File \u001b[0;32m~/miniforge3/envs/gameAI/lib/python3.8/site-packages/stable_baselines3/ppo/ppo.py:94\u001b[0m, in \u001b[0;36mPPO.__init__\u001b[0;34m(self, policy, env, learning_rate, n_steps, batch_size, n_epochs, gamma, gae_lambda, clip_range, clip_range_vf, ent_coef, vf_coef, max_grad_norm, use_sde, sde_sample_freq, target_kl, tensorboard_log, create_eval_env, policy_kwargs, verbose, seed, device, _init_setup_model)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[1;32m     68\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m     69\u001b[0m     policy: Union[\u001b[39mstr\u001b[39m, Type[ActorCriticPolicy]],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     91\u001b[0m     _init_setup_model: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m     92\u001b[0m ):\n\u001b[0;32m---> 94\u001b[0m     \u001b[39msuper\u001b[39;49m(PPO, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\n\u001b[1;32m     95\u001b[0m         policy,\n\u001b[1;32m     96\u001b[0m         env,\n\u001b[1;32m     97\u001b[0m         learning_rate\u001b[39m=\u001b[39;49mlearning_rate,\n\u001b[1;32m     98\u001b[0m         n_steps\u001b[39m=\u001b[39;49mn_steps,\n\u001b[1;32m     99\u001b[0m         gamma\u001b[39m=\u001b[39;49mgamma,\n\u001b[1;32m    100\u001b[0m         gae_lambda\u001b[39m=\u001b[39;49mgae_lambda,\n\u001b[1;32m    101\u001b[0m         ent_coef\u001b[39m=\u001b[39;49ment_coef,\n\u001b[1;32m    102\u001b[0m         vf_coef\u001b[39m=\u001b[39;49mvf_coef,\n\u001b[1;32m    103\u001b[0m         max_grad_norm\u001b[39m=\u001b[39;49mmax_grad_norm,\n\u001b[1;32m    104\u001b[0m         use_sde\u001b[39m=\u001b[39;49muse_sde,\n\u001b[1;32m    105\u001b[0m         sde_sample_freq\u001b[39m=\u001b[39;49msde_sample_freq,\n\u001b[1;32m    106\u001b[0m         tensorboard_log\u001b[39m=\u001b[39;49mtensorboard_log,\n\u001b[1;32m    107\u001b[0m         policy_kwargs\u001b[39m=\u001b[39;49mpolicy_kwargs,\n\u001b[1;32m    108\u001b[0m         verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m    109\u001b[0m         device\u001b[39m=\u001b[39;49mdevice,\n\u001b[1;32m    110\u001b[0m         create_eval_env\u001b[39m=\u001b[39;49mcreate_eval_env,\n\u001b[1;32m    111\u001b[0m         seed\u001b[39m=\u001b[39;49mseed,\n\u001b[1;32m    112\u001b[0m         _init_setup_model\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    113\u001b[0m         supported_action_spaces\u001b[39m=\u001b[39;49m(\n\u001b[1;32m    114\u001b[0m             spaces\u001b[39m.\u001b[39;49mBox,\n\u001b[1;32m    115\u001b[0m             spaces\u001b[39m.\u001b[39;49mDiscrete,\n\u001b[1;32m    116\u001b[0m             spaces\u001b[39m.\u001b[39;49mMultiDiscrete,\n\u001b[1;32m    117\u001b[0m             spaces\u001b[39m.\u001b[39;49mMultiBinary,\n\u001b[1;32m    118\u001b[0m         ),\n\u001b[1;32m    119\u001b[0m     )\n\u001b[1;32m    121\u001b[0m     \u001b[39m# Sanity check, otherwise it will lead to noisy gradient and NaN\u001b[39;00m\n\u001b[1;32m    122\u001b[0m     \u001b[39m# because of the advantage normalization\u001b[39;00m\n\u001b[1;32m    123\u001b[0m     \u001b[39massert\u001b[39;00m (\n\u001b[1;32m    124\u001b[0m         batch_size \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    125\u001b[0m     ), \u001b[39m\"\u001b[39m\u001b[39m`batch_size` must be greater than 1. See https://github.com/DLR-RM/stable-baselines3/issues/440\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[0;32m~/miniforge3/envs/gameAI/lib/python3.8/site-packages/stable_baselines3/common/on_policy_algorithm.py:77\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.__init__\u001b[0;34m(self, policy, env, learning_rate, n_steps, gamma, gae_lambda, ent_coef, vf_coef, max_grad_norm, use_sde, sde_sample_freq, policy_base, tensorboard_log, create_eval_env, monitor_wrapper, policy_kwargs, verbose, seed, device, _init_setup_model, supported_action_spaces)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[1;32m     53\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m     54\u001b[0m     policy: Union[\u001b[39mstr\u001b[39m, Type[ActorCriticPolicy]],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     74\u001b[0m     supported_action_spaces: Optional[Tuple[gym\u001b[39m.\u001b[39mspaces\u001b[39m.\u001b[39mSpace, \u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m     75\u001b[0m ):\n\u001b[0;32m---> 77\u001b[0m     \u001b[39msuper\u001b[39;49m(OnPolicyAlgorithm, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\n\u001b[1;32m     78\u001b[0m         policy\u001b[39m=\u001b[39;49mpolicy,\n\u001b[1;32m     79\u001b[0m         env\u001b[39m=\u001b[39;49menv,\n\u001b[1;32m     80\u001b[0m         policy_base\u001b[39m=\u001b[39;49mpolicy_base,\n\u001b[1;32m     81\u001b[0m         learning_rate\u001b[39m=\u001b[39;49mlearning_rate,\n\u001b[1;32m     82\u001b[0m         policy_kwargs\u001b[39m=\u001b[39;49mpolicy_kwargs,\n\u001b[1;32m     83\u001b[0m         verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m     84\u001b[0m         device\u001b[39m=\u001b[39;49mdevice,\n\u001b[1;32m     85\u001b[0m         use_sde\u001b[39m=\u001b[39;49muse_sde,\n\u001b[1;32m     86\u001b[0m         sde_sample_freq\u001b[39m=\u001b[39;49msde_sample_freq,\n\u001b[1;32m     87\u001b[0m         create_eval_env\u001b[39m=\u001b[39;49mcreate_eval_env,\n\u001b[1;32m     88\u001b[0m         support_multi_env\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     89\u001b[0m         seed\u001b[39m=\u001b[39;49mseed,\n\u001b[1;32m     90\u001b[0m         tensorboard_log\u001b[39m=\u001b[39;49mtensorboard_log,\n\u001b[1;32m     91\u001b[0m         supported_action_spaces\u001b[39m=\u001b[39;49msupported_action_spaces,\n\u001b[1;32m     92\u001b[0m     )\n\u001b[1;32m     94\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_steps \u001b[39m=\u001b[39m n_steps\n\u001b[1;32m     95\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgamma \u001b[39m=\u001b[39m gamma\n",
      "File \u001b[0;32m~/miniforge3/envs/gameAI/lib/python3.8/site-packages/stable_baselines3/common/base_class.py:106\u001b[0m, in \u001b[0;36mBaseAlgorithm.__init__\u001b[0;34m(self, policy, env, policy_base, learning_rate, policy_kwargs, tensorboard_log, verbose, device, support_multi_env, create_eval_env, monitor_wrapper, seed, use_sde, sde_sample_freq, supported_action_spaces)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[1;32m     87\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m     88\u001b[0m     policy: Type[BasePolicy],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    102\u001b[0m     supported_action_spaces: Optional[Tuple[gym\u001b[39m.\u001b[39mspaces\u001b[39m.\u001b[39mSpace, \u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    103\u001b[0m ):\n\u001b[1;32m    105\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(policy, \u001b[39mstr\u001b[39m) \u001b[39mand\u001b[39;00m policy_base \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 106\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpolicy_class \u001b[39m=\u001b[39m get_policy_from_name(policy_base, policy)\n\u001b[1;32m    107\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    108\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpolicy_class \u001b[39m=\u001b[39m policy\n",
      "File \u001b[0;32m~/miniforge3/envs/gameAI/lib/python3.8/site-packages/stable_baselines3/common/policies.py:914\u001b[0m, in \u001b[0;36mget_policy_from_name\u001b[0;34m(base_policy_type, name)\u001b[0m\n\u001b[1;32m    912\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mError: the policy type \u001b[39m\u001b[39m{\u001b[39;00mbase_policy_type\u001b[39m}\u001b[39;00m\u001b[39m is not registered!\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    913\u001b[0m \u001b[39mif\u001b[39;00m name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m _policy_registry[base_policy_type]:\n\u001b[0;32m--> 914\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\n\u001b[1;32m    915\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mError: unknown policy type \u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    916\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mthe only registed policy type are: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlist\u001b[39m(_policy_registry[base_policy_type]\u001b[39m.\u001b[39mkeys())\u001b[39m}\u001b[39;00m\u001b[39m!\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    917\u001b[0m     )\n\u001b[1;32m    918\u001b[0m \u001b[39mreturn\u001b[39;00m _policy_registry[base_policy_type][name]\n",
      "\u001b[0;31mKeyError\u001b[0m: \"Error: unknown policy type CNNPolicy,the only registed policy type are: ['MlpPolicy', 'CnnPolicy', 'MultiInputPolicy']!\""
     ]
    }
   ],
   "source": [
    "log_path = \"./Training/Logs/\"\n",
    "model = PPO(\"CNNPolicy\", env, verbose=1, tensorboard_log=log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.learn(total_timesteps=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_path = \"./Training/Models/PPO_Snake_Model\"\n",
    "model.save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gameAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "15f7727ce8d2895f6ec9d60196c112ed0686943606574ac3f5c99d53edfa75c1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
